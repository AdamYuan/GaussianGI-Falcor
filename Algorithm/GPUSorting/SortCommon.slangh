/******************************************************************************
 * GPUSorting
 *
 * SPDX-License-Identifier: MIT
 * Copyright Thomas Smith 4/28/2024
 * https://github.com/b0nes164/GPUSorting
 *
 ******************************************************************************/
#pragma once

#ifndef SORT_PAIRS
#define SORT_PAIRS 0
#endif

#ifndef SORT_INDIRECT
#define SORT_INDIRECT 1
#endif

#ifndef LANES_PER_WAVE
#define LANES_PER_WAVE 32
#endif

#if LANES_PER_WAVE < 16
#error WaveGetLaneCount() < 16 not supported
#endif

#define KEYS_PER_THREAD 15U
#define D_DIM 256U
#define PART_SIZE 3840U
#define D_TOTAL_SMEM 4096U

#define MAX_DISPATCH_DIM 65535U // The max value of any given dispatch dimension
#define RADIX 256U              // Number of digit bins
#define RADIX_MASK 255U         // Mask of digit bins
#define HALF_RADIX 128U         // For smaller waves where bit packing is necessary
#define HALF_MASK 127U          // ''
#define RADIX_LOG 8U            // log2(RADIX)
#define RADIX_PASSES 4U         //(Key width) / RADIX_LOG

uniform uint e_radixShift;

#if SORT_INDIRECT == 1
static uint e_numKeys;
uniform uint e_numKeyIndex;
StructuredBuffer<uint> b_numKeys;
RWStructuredBuffer<uint> b_indirect;
#else
uniform uint e_numKeys;
#endif

static uint e_threadBlocks;

void Prepare(uint blockSize)
{
#if SORT_INDIRECT == 1
    e_numKeys = b_numKeys[e_numKeyIndex];
#endif
    e_threadBlocks = (e_numKeys + blockSize - 1) / blockSize;
}

RWStructuredBuffer<uint> b_sort;
RWStructuredBuffer<uint> b_alt;

#if SORT_PAIRS == 1
RWStructuredBuffer<uint> b_sortPayload;
RWStructuredBuffer<uint> b_altPayload;
#endif

groupshared uint g_d[D_TOTAL_SMEM]; // Shared memory for DigitBinningPass and DownSweep kernels

struct KeyStruct
{
    uint k[KEYS_PER_THREAD];
};

struct OffsetStruct
{
    // #if defined(ENABLE_16_BIT)
    uint16_t o[KEYS_PER_THREAD];
    /* #else
        uint o[KEYS_PER_THREAD];
    #endif */
};

struct DigitStruct
{
    // #if defined(ENABLE_16_BIT)
    uint16_t d[KEYS_PER_THREAD];
    /* #else
        uint d[KEYS_PER_THREAD];
    #endif */
};

//*****************************************************************************
// HELPER FUNCTIONS
//*****************************************************************************
inline uint getWaveIndex(uint gtid)
{
    return gtid / LANES_PER_WAVE;
}

inline uint getWaveCountPass()
{
    return D_DIM / LANES_PER_WAVE;
}

inline uint ExtractDigit(uint key)
{
    return key >> e_radixShift & RADIX_MASK;
}

inline uint ExtractDigit(uint key, uint shift)
{
    return key >> shift & RADIX_MASK;
}

inline uint ExtractPackedIndex(uint key)
{
    return key >> (e_radixShift + 1) & HALF_MASK;
}

inline uint ExtractPackedShift(uint key)
{
    return bool(key >> e_radixShift & 1) ? 16 : 0;
}

inline uint ExtractPackedValue(uint packed, uint key)
{
    return packed >> ExtractPackedShift(key) & 0xffff;
}

inline uint SubPartSizeWGE16()
{
    return KEYS_PER_THREAD * LANES_PER_WAVE;
}

inline uint SharedOffsetWGE16(uint gtid)
{
    return WaveGetLaneIndex() + getWaveIndex(gtid) * SubPartSizeWGE16();
}

inline uint DeviceOffsetWGE16(uint gtid, uint partIndex)
{
    return SharedOffsetWGE16(gtid) + partIndex * PART_SIZE;
}

inline uint GlobalHistOffset()
{
    return e_radixShift << 5;
}

inline uint WaveHistsSizeWGE16()
{
    return D_DIM / LANES_PER_WAVE * RADIX;
}

//*****************************************************************************
// FUNCTIONS COMMON TO THE DOWNSWEEP / DIGIT BINNING PASS
//*****************************************************************************
// If the size of  a wave is too small, we do not have enough space in
// shared memory to assign a histogram to each wave, so instead,
// some operations are peformed serially.
inline void ClearWaveHists(uint gtid)
{
    const uint histsEnd = WaveHistsSizeWGE16();
    for (uint i = gtid; i < histsEnd; i += D_DIM)
        g_d[i] = 0;
}

inline void LoadKey(inout uint key, uint index)
{
    key = b_sort[index];
}

inline void LoadDummyKey(inout uint key)
{
    key = 0xffffffff;
}

inline KeyStruct LoadKeysWGE16(uint gtid, uint partIndex)
{
    KeyStruct keys;
    [unroll]
    for (uint i = 0, t = DeviceOffsetWGE16(gtid, partIndex); i < KEYS_PER_THREAD; ++i, t += LANES_PER_WAVE)
    {
        LoadKey(keys.k[i], t);
    }
    return keys;
}

inline KeyStruct LoadKeysPartialWGE16(uint gtid, uint partIndex)
{
    KeyStruct keys;
    [unroll]
    for (uint i = 0, t = DeviceOffsetWGE16(gtid, partIndex); i < KEYS_PER_THREAD; ++i, t += LANES_PER_WAVE)
    {
        if (t < e_numKeys)
            LoadKey(keys.k[i], t);
        else
            LoadDummyKey(keys.k[i]);
    }
    return keys;
}

inline uint WaveFlagsWGE16()
{
    return bool(LANES_PER_WAVE & 31) ? (1U << LANES_PER_WAVE) - 1 : 0xffffffff;
}

inline void WarpLevelMultiSplitWGE16(uint key, uint waveParts, inout uint4 waveFlags)
{
    [unroll]
    for (uint k = 0; k < RADIX_LOG; ++k)
    {
        const bool t = bool(key >> (k + e_radixShift) & 1);
        const uint4 ballot = WaveActiveBallot(t);
        for (uint wavePart = 0; wavePart < waveParts; ++wavePart)
            waveFlags[wavePart] &= (t ? 0 : 0xffffffff) ^ ballot[wavePart];
    }
}

inline void CountPeerBits(inout uint peerBits, inout uint totalBits, uint4 waveFlags, uint waveParts)
{
    for (uint wavePart = 0; wavePart < waveParts; ++wavePart)
    {
        if (WaveGetLaneIndex() >= wavePart * 32)
        {
            const uint ltMask = WaveGetLaneIndex() >= (wavePart + 1) * 32 ? 0xffffffff : (1U << (WaveGetLaneIndex() & 31)) - 1;
            peerBits += countbits(waveFlags[wavePart] & ltMask);
        }
        totalBits += countbits(waveFlags[wavePart]);
    }
}

inline uint FindLowestRankPeer(uint4 waveFlags, uint waveParts)
{
    uint lowestRankPeer = 0;
    for (uint wavePart = 0; wavePart < waveParts; ++wavePart)
    {
        uint fbl = firstbitlow(waveFlags[wavePart]);
        if (fbl == 0xffffffff)
            lowestRankPeer += 32;
        else
            return lowestRankPeer + fbl;
    }
    return 0; // will never happen
}

inline OffsetStruct RankKeysWGE16(uint gtid, KeyStruct keys)
{
    OffsetStruct offsets;
    const uint waveParts = (LANES_PER_WAVE + 31) / 32;
    [unroll]
    for (uint i = 0; i < KEYS_PER_THREAD; ++i)
    {
        uint4 waveFlags = WaveFlagsWGE16();
        WarpLevelMultiSplitWGE16(keys.k[i], waveParts, waveFlags);

        const uint index = ExtractDigit(keys.k[i]) + (getWaveIndex(gtid.x) * RADIX);
        const uint lowestRankPeer = FindLowestRankPeer(waveFlags, waveParts);

        uint peerBits = 0;
        uint totalBits = 0;
        CountPeerBits(peerBits, totalBits, waveFlags, waveParts);

        uint preIncrementVal;
        if (peerBits == 0)
            InterlockedAdd(g_d[index], totalBits, preIncrementVal);
        offsets.o[i] = uint16_t(WaveReadLaneAt(preIncrementVal, lowestRankPeer) + peerBits);
    }

    return offsets;
}

inline uint WaveHistInclusiveScanCircularShiftWGE16(uint gtid)
{
    uint histReduction = g_d[gtid];
    for (uint i = gtid + RADIX; i < WaveHistsSizeWGE16(); i += RADIX)
    {
        histReduction += g_d[i];
        g_d[i] = histReduction - g_d[i];
    }
    return histReduction;
}

inline void WaveHistReductionExclusiveScanWGE16(uint gtid, uint histReduction)
{
    if (gtid < RADIX)
    {
        const uint laneMask = LANES_PER_WAVE - 1;
        g_d[((WaveGetLaneIndex() + 1) & laneMask) + (gtid & ~laneMask)] = histReduction;
    }
    GroupMemoryBarrierWithGroupSync();

    if (gtid < RADIX / LANES_PER_WAVE)
    {
        g_d[gtid * LANES_PER_WAVE] = WavePrefixSum(g_d[gtid * LANES_PER_WAVE]);
    }
    GroupMemoryBarrierWithGroupSync();

    if (gtid < RADIX && WaveGetLaneIndex() > 0)
        g_d[gtid] += WaveReadLaneAt(g_d[gtid - 1], 1);
}

// inclusive/exclusive prefix sum up the histograms,
// use a blelloch scan for in place packed exclusive

inline void UpdateOffsetsWGE16(uint gtid, inout OffsetStruct offsets, KeyStruct keys)
{
    if (gtid >= LANES_PER_WAVE)
    {
        const uint t = getWaveIndex(gtid) * RADIX;
        [unroll]
        for (uint i = 0; i < KEYS_PER_THREAD; ++i)
        {
            const uint t2 = ExtractDigit(keys.k[i]);
            offsets.o[i] += uint16_t(g_d[t2 + t] + g_d[t2]);
        }
    }
    else
    {
        [unroll]
        for (uint i = 0; i < KEYS_PER_THREAD; ++i)
            offsets.o[i] += uint16_t(g_d[ExtractDigit(keys.k[i])]);
    }
}

inline void ScatterKeysShared(OffsetStruct offsets, KeyStruct keys)
{
    [unroll]
    for (uint i = 0; i < KEYS_PER_THREAD; ++i)
        g_d[uint(offsets.o[i])] = keys.k[i];
}

inline void WriteKey(uint deviceIndex, uint groupSharedIndex)
{
    b_alt[deviceIndex] = g_d[groupSharedIndex];
}

inline void ScatterPayloadsShared(OffsetStruct offsets, KeyStruct payloads)
{
    ScatterKeysShared(offsets, payloads);
}

#if SORT_PAIRS == 1

inline void LoadPayload(inout uint payload, uint deviceIndex)
{
    payload = b_sortPayload[deviceIndex];
}

inline void WritePayload(uint deviceIndex, uint groupSharedIndex)
{
    b_altPayload[deviceIndex] = g_d[groupSharedIndex];
}

#endif

//*****************************************************************************
// SCATTERING: FULL PARTITIONS
//*****************************************************************************
// KEYS ONLY
inline void ScatterKeysOnlyDeviceAscending(uint gtid)
{
    for (uint i = gtid; i < PART_SIZE; i += D_DIM)
        WriteKey(g_d[ExtractDigit(g_d[i]) + PART_SIZE] + i, i);
}

inline void ScatterKeysOnlyDevice(uint gtid)
{
    ScatterKeysOnlyDeviceAscending(gtid);
}

// KEY VALUE PAIRS
inline void ScatterPairsKeyPhaseAscending(uint gtid, inout DigitStruct digits)
{
    [unroll]
    for (uint i = 0, t = gtid; i < KEYS_PER_THREAD; ++i, t += D_DIM)
    {
        digits.d[i] = uint16_t(ExtractDigit(g_d[t]));
        WriteKey(g_d[uint(digits.d[i]) + PART_SIZE] + t, t);
    }
}

#if SORT_PAIRS == 1

inline void LoadPayloadsWGE16(uint gtid, uint partIndex, inout KeyStruct payloads)
{
    [unroll]
    for (uint i = 0, t = DeviceOffsetWGE16(gtid, partIndex); i < KEYS_PER_THREAD; ++i, t += LANES_PER_WAVE)
    {
        LoadPayload(payloads.k[i], t);
    }
}

inline void ScatterPayloadsAscending(uint gtid, DigitStruct digits)
{
    [unroll]
    for (uint i = 0, t = gtid; i < KEYS_PER_THREAD; ++i, t += D_DIM)
        WritePayload(g_d[uint(digits.d[i]) + PART_SIZE] + t, t);
}

inline void ScatterPairsDevice(uint gtid, uint partIndex, OffsetStruct offsets)
{
    DigitStruct digits;
    ScatterPairsKeyPhaseAscending(gtid, digits);
    GroupMemoryBarrierWithGroupSync();

    KeyStruct payloads;
    LoadPayloadsWGE16(gtid, partIndex, payloads);
    ScatterPayloadsShared(offsets, payloads);
    GroupMemoryBarrierWithGroupSync();

    ScatterPayloadsAscending(gtid, digits);
}

#endif

inline void ScatterDevice(uint gtid, uint partIndex, OffsetStruct offsets)
{
#if SORT_PAIRS == 1
    ScatterPairsDevice(gtid, partIndex, offsets);
#else
    ScatterKeysOnlyDevice(gtid);
#endif
}

//*****************************************************************************
// SCATTERING: PARTIAL PARTITIONS
//*****************************************************************************
// KEYS ONLY
inline void ScatterKeysOnlyDevicePartialAscending(uint gtid, uint finalPartSize)
{
    for (uint i = gtid; i < PART_SIZE; i += D_DIM)
    {
        if (i < finalPartSize)
            WriteKey(g_d[ExtractDigit(g_d[i]) + PART_SIZE] + i, i);
    }
}

inline void ScatterKeysOnlyDevicePartial(uint gtid, uint partIndex)
{
    const uint finalPartSize = e_numKeys - partIndex * PART_SIZE;
    ScatterKeysOnlyDevicePartialAscending(gtid, finalPartSize);
}

// KEY VALUE PAIRS
#if SORT_PAIRS == 1

inline void ScatterPairsKeyPhaseAscendingPartial(uint gtid, uint finalPartSize, inout DigitStruct digits)
{
    [unroll]
    for (uint i = 0, t = gtid; i < KEYS_PER_THREAD; ++i, t += D_DIM)
    {
        if (t < finalPartSize)
        {
            digits.d[i] = uint16_t(ExtractDigit(g_d[t]));
            WriteKey(g_d[uint(digits.d[i]) + PART_SIZE] + t, t);
        }
    }
}

inline void LoadPayloadsPartialWGE16(uint gtid, uint partIndex, inout KeyStruct payloads)
{
    [unroll]
    for (uint i = 0, t = DeviceOffsetWGE16(gtid, partIndex); i < KEYS_PER_THREAD; ++i, t += LANES_PER_WAVE)
    {
        if (t < e_numKeys)
            LoadPayload(payloads.k[i], t);
    }
}

inline void ScatterPayloadsAscendingPartial(uint gtid, uint finalPartSize, DigitStruct digits)
{
    [unroll]
    for (uint i = 0, t = gtid; i < KEYS_PER_THREAD; ++i, t += D_DIM)
    {
        if (t < finalPartSize)
            WritePayload(g_d[uint(digits.d[i]) + PART_SIZE] + t, t);
    }
}

inline void ScatterPairsDevicePartial(uint gtid, uint partIndex, OffsetStruct offsets)
{
    DigitStruct digits;
    const uint finalPartSize = e_numKeys - partIndex * PART_SIZE;
    ScatterPairsKeyPhaseAscendingPartial(gtid, finalPartSize, digits);
    GroupMemoryBarrierWithGroupSync();

    KeyStruct payloads;
    LoadPayloadsPartialWGE16(gtid, partIndex, payloads);
    ScatterPayloadsShared(offsets, payloads);
    GroupMemoryBarrierWithGroupSync();

    ScatterPayloadsAscendingPartial(gtid, finalPartSize, digits);
}

#endif

inline void ScatterDevicePartial(uint gtid, uint partIndex, OffsetStruct offsets)
{
#if SORT_PAIRS == 1
    ScatterPairsDevicePartial(gtid, partIndex, offsets);
#else
    ScatterKeysOnlyDevicePartial(gtid, partIndex);
#endif
}
